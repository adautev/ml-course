---
title: "R ML excercises"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: inline
---

Load necessary data.

```{r}

check.packages <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}
packages<-c("ggplot2", "tidyverse", "lubridate", "caret", "dslabs", "plyr", "dplyr", "HistData","pdftools","devtools", "rpart")
check.packages(packages)
data("heights")
data("iris")
devtools::install_bioc("genefilter")
library(genefilter)

```

Split by predictors and predicted value.

```{r}
y<-heights$sex
x<-heights$height
```

Split tet into training and evaluation.

```{r echo=TRUE}
set.seed(2)
test_index <- createDataPartition(y, times=1, p=0.5, list=FALSE)
test_set <- heights[test_index, ]
training_set <- heights[-test_index, ]

```

Determine y_hat for the test set just by guessing.
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
y_hat <- sample(c("Male", "Female"), size = length(test_index), replace=TRUE)
mean(y_hat == test_set$sex)
```
Consider the bias, that males are higher than females.
```{r}
heights %>% group_by(sex) %>% summarize(mean(height), sd(height))
```

Naively predict Male and Female based on height. (;

```{r}
y_hat <- ifelse(x>62, "Male", "Female") %>% factor(levels=levels(test_set$sex))
table(y_hat)
print("Mean accurate answers:")
mean(y==y_hat)
```

Check other vut off values, except for 62:
```{r}
cutoff <- seq(61, 70)
accuracy <- map_dbl(cutoff, function(cutoff_value) {
  y_hat <- ifelse(test_set$height>cutoff_value, "Male", "Female") 
  mean(y_hat == test_set$sex)
})
ggplot(data=data.frame(cutoff=cutoff, accuracy=accuracy), aes(x=cutoff, y=accuracy)) + geom_line() + geom_point() + ggtitle("Accuracy distribution depending on 61:70 cutoff")
```

Check out the accuracy by sex
```{r}
heights %>% mutate(y_hat = y_hat) %>%
  group_by(sex) %>%
  summarize(accuracy = mean(y_hat == sex))
```

Notes on confusion matrixes.
**Sensitivity**: The avility of algo to predict positive outcome, when the actual outcome is positive: P($\widehat{Y}$=1|Y=1). Also Sensitivity = TP/(TP + FN) 
**Specificity**: The avility of algo to **not** predict positive outcome, when the actual outcome is **not** positive: P($\widehat{Y}$=0|Y=0). Also Specificity = TN/(TN + FP) 

```{r}
confusionMatrix(data=y_hat, reference=heights$sex)
```

Get F_1 harmonised score based on the cutoff:
```{r}
F_1 <- map_dbl(cutoff, function(cutoff_value) {
  y_hat <- ifelse(test_set$height>cutoff_value, "Male", "Female") %>% factor(levels=levels(test_set$sex))
  F_meas(data = y_hat, reference = factor(test_set$sex))
})
ggplot(data=data.frame(cutoff=cutoff, F_meas=F_1), aes(x=cutoff, y=F_meas)) + geom_line() + geom_point() + ggtitle("Accuracy distribution depending on 61:70 cutoff")
```
#The test set bias
The test set has more males, so guessing 90% of the time males, would give a mean up to 72-3%:
```{r}
y_hat <- sample(c("Male","Female"), length(test_index), replace=TRUE, prob=c(0.9, 0.1)) %>% factor(levels = levels(test_set$sex))
mean(y_hat==test_set$sex)
```

#Receiver Operating Characteristics - the ROC curve
```{r}
cutoffs <-c(50, seq(60, 75), 80)
height_cutoff <- map_df(cutoffs, function (cutoff_value) {
    y_hat <- ifelse(test_set$height>cutoff_value, "Male", "Female") %>% factor(levels=levels(test_set$sex))
    list(method="Height cutoff",
         FPR = 1-specificity(y_hat, test_set$sex),
         TPR = sensitivity(y_hat, test_set$sex))
})
ggplot(data=height_cutoff, aes(x=FPR, y=TPR)) + geom_line() + geom_point() 
```
#Precision recall plot 
```{r echo=TRUE}
height_cutoff <- map_df(cutoffs, function (cutoff_value) {
    y_hat <- ifelse(test_set$height>cutoff_value, "Male", "Female") %>% factor(levels=levels(test_set$sex))
    list(method="Height cutoff",
         precision = precision(y_hat, test_set$sex),
         recall = sensitivity(y_hat, test_set$sex))
})
ggplot(data=height_cutoff, aes(x=recall, y=precision)) + geom_line() + geom_point() 
probs = seq(0.1, 0.9, by = 0.1)
guessing <- map_df(probs, function(p) {
  y_hat <- sample(c("Male", "Female"), length(test_index), replace=TRUE, prob=c(p, 1-p)) %>% factor(levels=levels(test_set$sex))
  list(method="Guessing",
       precision = precision(y_hat, test_set$sex),
       recall = sensitivity(y_hat, test_set$sex))
})
ggplot(data=height_cutoff, aes(x=recall, y=precision)) + geom_line() + geom_point() + geom_line(data=guessing, aes(x=recall, y=precision)) + geom_point(data=guessing, aes(x=recall, y=precision))
```

Excercises:
The following questions all ask you to work with the dataset described below.

The reported_heights and heights datasets were collected from three classes taught in the Departments of Computer Science and Biostatistics, as well as remotely through the Extension School. The Biostatistics class was taught in 2016 along with an online version offered by the Extension School. On 2016-01-25 at 8:15 AM, during one of the lectures, the instructors asked student to fill in the sex and height questionnaire that populated the reported_height dataset. The online students filled out the survey during the next few days, after the lecture was posted online. We can use this insight to define a variable which we will call type, to denote the type of student, inclass or online.

The code below sets up the dataset for you to analyze in the following exercises:

```{r}

library(dslabs)
library(dplyr)


data("reported_heights")

dat <- mutate(reported_heights, date_time = ymd_hms(time_stamp)) %>%
  filter(date_time >= make_date(2016, 01, 25) & date_time < make_date(2016, 02, 1)) %>%
  mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & between(minute(date_time), 15, 30), "inclass","online")) %>%
  select(sex, type)

y <- factor(dat$sex, c("Female", "Male"))
x <- dat$type
dat %>% group_by(type) %>% summarize(prop_female = mean(sex == "Female"))

```
If you used the type variable to predict sex, what would the prediction accuracy be?
```{r}

  y_hat <- ifelse(dat$type == "inclass", "Male", "Female") 
  mean(y_hat == dat$sex)

```
Write a line of code using the table function to show the confusion matrix, assuming the prediction is y_hat and the truth is y.

```{r}
 y_hat <- ifelse(dat$type == "inclass", "Female", "Male") %>% factor(levels=levels(y))
 
 confusionMatrix(y_hat, y)
```

#Comprehension Check: Practice with Machine Learning
We will practice building a machine learning algorithm using a new dataset, iris, that provides multiple predictors for us to use to train. To start, we will remove the setosa species and we will focus on the versicolor and virginica iris species using the following code:

library(caret)
data(iris)
iris <- iris[-which(iris$Species=='setosa'),]
y <- iris$Species

The following questions all involve work with this dataset.
First let us create an even split of the data into train and test partitions using createDataPartition. The code with a missing line is given below:

```{r}
data(iris)
iris <- iris[-which(iris$Species=='setosa'),]
y <- iris$Species

set.seed(2)
test_index <- createDataPartition(y,times=1,p=0.5,list=FALSE)
# line of code
#Which code should be used in place of # line of code above?
#Answer:
 
test <- iris[test_index,]
train <- iris[-test_index,]
```


Answers are displayed within the problem Review
Q2
1/1 point (graded)
Next we will figure out the singular feature in the dataset that yields the greatest overall accuracy. You can use the code from the introduction and from Q1 to start your analysis.

Using only the train iris data set, which of the following is the singular feature for which a smart cutoff (simple search) yields the greatest overall accuracy?
```{r}
foo <- function(x){
	rangedValues <- seq(range(x)[1],range(x)[2],by=0.1)
	sapply(rangedValues,function(i){
		y_hat <- ifelse(x>i,'virginica','versicolor')
		mean(y_hat==train$Species)
	})
}
predictions <- apply(train[,-5],2,foo)
sapply(predictions,max)	
```

Using the smart cutoff value calculated on the training data, what is the overall accuracy in the test data?

```{r}
foo <- function(x){
	rangedValues <- seq(range(x)[1],range(x)[2],by=0.1)
	sapply(rangedValues,function(i){
		y_hat <- ifelse(x>i,'virginica','versicolor')
		mean(y_hat==test$Species)
	})
}
predictions <- apply(test[,-5],2,foo)
sapply(predictions,max)	
```
Notice that we had an overall accuracy greater than 96% in the training data, but the overall accuracy was lower in the test data. This can happen often if we overtrain. In fact, it could be the case that a single feature is not the best choice. For example, a combination of features might be optimal. Using a single feature and optimizing the cutoff as we did on our training data can lead to overfitting.

Given that we know the test data, we can treat it like we did our training data to see if the same feature with a different cutoff will optimize our predictions.

Which feature best optimizes our overall accuracy? - Petal.Width = 0.94

Now we will perform some exploratory data analysis on the data.sqrt(mean((y_hat-test_set$y)^2))

Notice that Petal.Length and Petal.Width in combination could potentially be more information than either feature alone.

Optimize the combination of the cutoffs for Petal.Length and Petal.Width in the train data and report the overall accuracy when applied to the test dataset. For simplicity, create a rule that if either the length OR the width is greater than the length cutoff or the width cutoff then virginica or versicolor is called. (Note, the F1 will be similarly high in this example.)

What is the overall accuracy for the test data now?

```{r}
petalLengthRange <- seq(range(train[,3])[1],range(train[,3])[2],by=0.1)
petalWidthRange <- seq(range(train[,4])[1],range(train[,4])[2],by=0.1)
cutoffs <- expand.grid(petalLengthRange,petalWidthRange)

id <- sapply(seq(nrow(cutoffs)),function(i){
	y_hat <- ifelse(train[,3]>cutoffs[i,1] | train[,4]>cutoffs[i,2],'virginica','versicolor')
	mean(y_hat==train$Species)
	}) %>% which.max

optimalCutoff <- cutoffs[id,] %>% as.numeric
y_hat <- ifelse(test[,3]>optimalCutoff[1] & test[,4]>optimalCutoff[2],'virginica','versicolor')
mean(y_hat==test$Species)


```

#Conditional probabilities. 

For classes k = 1...K
We have observed values x1,....,xp
For covariates X1,.....,Xp

This *does not imply* that the outcome y will take a specific value, but implies there is a specific probability.
We denote the conditional probabilities for different classes:
Pr(Y = k|X1=x1,...Xp=p) for k=1,....,K 

To avoid expressing X1,...,Xp and x1,...xp, we denote:
X=(X1,...,Xp)
x=(x1,...,xp)

So! The *conditional probability* of being in class K is denoted:
p<sub>k</sub>(x) = P(Y|X=x) for k=1,.....K

We will be using the notation p(x) to represent conditional probabilities as functions.

The goal here is to predict the highest probability of outcome belonging to a given class, among p1(x),p2(x),....p<sub>K</sub>(x)

E.g. How close the maximum probability is to 1:
max p<sub>k</sub>(x) is to 1 *or*
 
how close our estimate
$\widehat{p}$<sub>k</sub>(x) is to p<sub>k</sub>(x)

#Loss function
For fixed samples of data, specificity, sensitivity, accuracy and F1 are used to describe algorithm properties.
For continuous outcomes, though we define a loss function, which is denoted as $${(\widehat{Y} - Y)}^2$$
where $\widehat{Y}$ is predictor and Y actual outcome.

Since there are tests with multiple observation, the mean squared error is a mean to determine accuracy (an the actual accuracy for binary outcomes)
$$\frac{1}{N}\sum_{i=1}^{n}{(\widehat{Y}_i - Y_i)}^2$$
The general goal is to build an algorithm that minimises the loss and have a value close or equal to 0.
We want the algorithm that minimizes the average of the squared loss accross many random samples.
$$E\{\frac{1}{N}\sum_{i=1}^{n}{(\widehat{Y}_i - Y_i)}^2\}$$

e.g. the expectation of squared error - *expected loss*

##The main task of machine learning is to use data to estimate conditional probabilities 
$$f(x)≡E(Y|X=X)$$
for any set of features $${x=(x_1,...,x_p)}$$
#Excercisessqrt(mean((y_hat-test_set$y)^2))
We have a hypothetical population of 1 million individuals with the following conditional probabilities as described below:
The test is positive 85% of the time when tested on a patient with the disease (high sensitivity): 

P(test+|disease)=0.85
The test is negative 90% of the time when tested on a healthy patient (high specificity): 

P(test−|heathy)=0.90
The disease is prevalent in about 2% of the community: 

P(disease)=0.02
Here is some sample code to get you started:
set.seed(1)

```{r}
disease <- sample(c(0,1), size=1e6, replace=TRUE, prob=c(0.98,0.02))
test <- rep(NA, 1e6)
test[disease==0] <- sample(c(0,1), size=sum(disease==0), replace=TRUE, prob=c(0.90,0.10))
test[disease==1] <- sample(c(0,1), size=sum(disease==1), replace=TRUE, prob=c(0.15, 0.85))

print('#Q1: What is the probability that a test is positive?')
mean(test==1)
print('#Q2: What is the probability that an individual has the disease if the test is negative?')
mean(disease[test==0])
print('#Q3: What is the probability that you have the disease if the test is positive?')
mean(disease[test==1]==1)
print('#Q4: If the test is positive, what is the relative risk of having the disease?')
mean(disease[test==1]==1)/mean(disease==1)
```

We are now going to write code to compute conditional probabilities for being male in the heights dataset. Round the heights to the closest inch. Plot the estimated conditional probability 
P(x)=Pr(Male|height=x)
for each x. 
```{r}
library(dslabs)
data("heights")
hey <- heights %>% 
	mutate(height = round(height)) %>%
	group_by(height) %>%
	#summarize(p = mean(sex == "Male")) #%>%
	qplot(height, p, data =.)
```

In the plot we just made in Q1 we see high variability for low values of height. This is because we have few data points. This time use the quantile (\ 0.1,0.2,\dots,0.9 \)and the cut function to assure each group has the same number of points. Note that for any numeric vector x, you can create groups based on quantiles like this: cut(x, quantile(x, seq(0, 1, 0.1)), include.lowest = TRUE). 
```{r}
ps <- seq(0, 1, 0.1)
heights %>% mutate(g = cut(height, quantile(height, ps), include.lowest = TRUE))
  
```

You can generate data from a bivariate normal distrubution using the MASS package using the following code. 

```{r}
Sigma <- 9*matrix(c(1,0.5,0.5,1), 2, 2)
dat <- MASS::mvrnorm(n = 10000, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))
plot(dat)
print("Using an approach similar to that used in the previous exercise, let's estimate the conditional expectations and make a plot. Part of the code has been provided for you: ")
ps <- seq(0, 1, 0.1)
dat %>% 
	mutate(g = cut(x, quantile(x, ps), include.lowest = TRUE)) %>%
  group_by(g) %>%
  summarize(y = mean(y), x = mean(x)) %>%
	qplot(x, y, data =.)
```

#Linear regression

```{r}
galton_heights <- GaltonFamilies %>% filter(childNum == 1 & gender == "male") %>% select (father, childHeight)
colnames(galton_heights)[colnames(galton_heights)=="childHeight"] <- "son"
y <- galton_heights$son
test_index <- createDataPartition(y, times = 1, p=0.5, list = FALSE)
train_set <- galton_heights %>% slice(-test_index)
test_set <- galton_heights %>% slice(test_index)
#Guessing
avg_height <- mean(train_set$son)
#Squared loss is: 
mean((avg_height - train_set$son)^2)
print("Getting intercept and slope:")
fit <-lm(son ~ father, data=train_set)
fit$coef
```

From the regression course:
$$f(x) = Pr(Y|X=x) = \beta_{0} + x*\beta_{1} => \widehat{f}(x) = 38 + 0.47x$$
Let's see how the sqared error results, instead of guessing:
```{r}
y_hat <- fit$coef[1] + fit$coef[2]*test_set$father
mean((y_hat - test_set$son)^2)
```

#The predict function
Replaces the formula for the regression line. 
```{r}
y_hat <- predict(fit, test_set)
```

#Excercises
Create a data set using the following code:
```{r}
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))
```

Use the caret package to partition the dataset into test and training sets of equal size. Train a linear model and calculate the RMSE. Repeat this exercise 100 times and report the mean and standard deviation of the RMSEs. (Hint: You can use the code shown in a previous course inside a call to replicate using a seed of 1.
```{r}

set.seed(1)
rmses <- replicate(100,  { 
  test_index <- createDataPartition(dat$y, times = 1, p=0.5, list = FALSE)
  train_set <- dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)
  fit <-lm(y ~ x, data=train_set)
  y_hat <- predict(fit, test_set)
  rmse <- sqrt(mean((y_hat - test_set$y)^2)) 
  rmse
}) 
mean(rmses)
sd(rmses)
```

Now we will repeat the above but using larger datasets. Repeat the previous exercise but for datasets with n <- c(100, 500, 1000, 5000, 10000). Save the average and standard deviation of RMSE from the 100 repetitions using a seed of 1. Hint: use the sapply or map functions.

```{r}
set.seed(1)
map(c(100, 500, 1000, 5000, 10000), function(dataSetLength) {
  

  Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
  dat <- MASS::mvrnorm(n = dataSetLength, c(69, 69), Sigma) %>%
  data.frame() %>% setNames(c("x", "y"))
  rmses <- replicate(100,  { 
    test_index <- createDataPartition(dat$y, times = 1, p=0.5, list = FALSE)
    train_set <- dat %>% slice(-test_index)
    test_set <- dat %>% slice(test_index)
    fit <-lm(y ~ x, data=train_set)
    y_hat <- predict(fit, test_set)
    rmse <- sqrt(mean((y_hat - test_set$y)^2)) 
    rmse
  })
  
  print(paste("Items: ", toString(dataSetLength)))
  print(mean(rmses))
  print(sd(rmses))
})
```
Now repeat the exercise from Q1, this time making the correlation between x and y larger, as in the following code:

```{r}
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
data.frame() %>% setNames(c("x", "y"))
set.seed(1)
rmses <- replicate(100,  { 
  test_index <- createDataPartition(dat$y, times = 1, p=0.5, list = FALSE)
  train_set <- dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)
  fit <-lm(y ~ x, data=train_set)
  y_hat <- predict(fit, test_set)
  rmse <- sqrt(mean((y_hat - test_set$y)^2)) 
  rmse
}) 
mean(rmses)
sd(rmses)
```

Create a data set using the following code.
Note that y is correlated with both x_1 and x_2 but the two predictors are independent of each other, as seen by cor(dat).

Use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2 and both x_1 and x_2. Train a linear model for each.

Which of the three models performs the best (has the lowest RMSE)?
```{r}
set.seed(1)
n <- 1000
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))

set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_1 + x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

Repeat the exercise from q6 but now create an example in which x_1 and x_2 are highly correlated.
```{r}
set.seed(1)
n <- 1000
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))

set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_1 + x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

#Logistic regression excercise
Define a dataset using the following code:

```{r}
set.seed(2)
make_data <- function(n = 1000, p = 0.5, 
				mu_0 = 0, mu_1 = 2, 
				sigma_0 = 1,  sigma_1 = 1){

y <- rbinom(n, 1, p)
f_0 <- rnorm(n, mu_0, sigma_0)
f_1 <- rnorm(n, mu_1, sigma_1)
x <- ifelse(y == 1, f_1, f_0)
  
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

list(train = data.frame(x = x, y = as.factor(y)) %>% slice(-test_index),
	test = data.frame(x = x, y = as.factor(y)) %>% slice(test_index))
}
dat <- make_data()
dat$train %>% ggplot(aes(x, color = y)) + geom_density()
set.seed(1)
delta <- seq(0, 3, len = 25)
res <- sapply(delta, function(d){
	dat <- make_data(mu_1 = d)
	fit_glm <- dat$train %>% glm(y ~ x, family = "binomial", data = .)
	y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) %>% factor(levels = c(0, 1))
	mean(y_hat_glm == dat$test$y)
})
qplot(delta, res)
```
#Smoothing
##Problem with one predictor
Let's estimate the time trend in the popular vote from the 2008 election.
```{r}
data("polls_2008")
qplot(day, margin, data=polls_2008)
```
###Let's predict y given the day x.
This is the mathematical model for the observed poll margin Y.
$Y_i=f(x_i)+\epsilon_i$
The conditional expectation is:
f(x)=E(Y|X=x)
Linear regression shows:
```{r}
day_margin_fit <-lm(margin ~ day, data=polls_2008)
polls_2008_scatter_data <- polls_2008 %>% mutate(resid=resid(day_margin_fit), color=ifelse(resid(day_margin_fit) >0,"+","-"), fitted=fitted(day_margin_fit))
ggplot(aes(day, margin, color=color), data=polls_2008_scatter_data) + geom_point() + geom_abline(intercept = day_margin_fit$coefficients[1], slope=day_margin_fit$coefficients[2])
    
#
```
##Bin smoothing
In order to evaluate smoothing method, assumptions are needed about the data. In the current case, we determine that data grows slowly in the week, so we get wednesday and evaluate the margin for the week.
We consider $f(x)=\mu$ if $|x_i-x_0| \leq 3.5$. When $x_0$ is a growth baseline (let's say middle of the week).
In sych case $E[Y_i|X_i=x_i] \approx \mu$ if $|x_i-x_0| \leq 3.5$
3.5 is called window size/bandwith/span.

If we define $A_0$ as a set of indexes i, such that $|x_i-x_0| \leq 3.5$,and $N_0$ as the number of indexes in $A_0$, then the estimate is given by:

$\widehat{f}(x_0) = 1/N_0_\sum{_{i \in A_0} Y_i$ - the average in the window
 
The results are:
```{r}
span <- 7 
fit <- with(polls_2008, ksmooth(day, margin, x.points = day, kernel="normal", bandwidth = span))
polls_2008 %>% mutate(smooth = fit$y) %>% ggplot(aes(day, margin)) + geom_point(size = 3, alpha = .5, color = "grey") + geom_line(aes(day, smooth), color="red")
```
##Local Weighted regression(loess)
Instead of assuming that a function is approximately constant in a window, we decide function is locally linear (Taylor's theorem). This way larger windows can be considered. Let's start with 3 week window.
Here's the model.
$E[Y_i|X_i=x_i] = \beta_0 + \beta_1(x_i-x_0)$ if $|x_i-x_0| \leq 10.5$

The final estimate is given as:
```{r}
total_days <-diff(range(polls_2008$day))
span <- 21/total_days
fit <- loess(margin~day, degree=1, span=span, data=polls_2008)
polls_2008 %>% mutate(smooth=fit$fitted) %>% ggplot(aes(day, margin)) + geom_point(size=3, alpha=.5, color="grey") + geom_line(aes(day, smooth), color="red")
```
###Excercise
Suppose we want to predict 2s and 7s in the mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power.

In fact, if we fit a regular logistic regression the coefficient for x_2 is not significant!

This can be seen using this code:

```{r}
library(broom)
mnist_27$train %>% glm(y ~ x_2, family = "binomial", data = .) %>% tidy()
qplot(x_2, y, data = mnist_27$train)
#Fit a loess line to the data above and plot the results. What do you observe?
mnist_27$train %>% 
	mutate(y = ifelse(y=="7", 1, 0)) %>%
	ggplot(aes(x_2, y)) + 
	geom_smooth(method = "loess")

```

#Matrices
1000 predictors and 1000 labels from the mnist package:
```{r}
mnist<-read_mnist()
predictors <-mnist$train$images[1:1000,]
labels <-mnist$train$labels[1:1000]

```

##Binding matrices from vectors
```{r}
#defining a 2x3 matrix
v1<-c("boom", "bam", "bim")
v2<-c("buum", "bom", "bum")
cbind(v1, v2)
```
##Summaries
```{r}
pixelDarknessTotal <- rowSums(predictors)
pixelDarknessAvg <- rowMeans(predictors)
print("See distribution of pixel darkess.")
data.frame(labels=as.factor(labels), row_averages=pixelDarknessAvg) %>% ggplot(aes(labels,row_averages)) + geom_boxplot() + labs(title="Pixel darkness distribution by row")
```
##Indexing and binarizing data
```{r}
print("Binarizing all pixels < 125.5 to 0 and > 125.5 as 1")
bin_preidctors<-predictors;
bin_preidctors[bin_preidctors<255/2] <-0;
bin_preidctors[bin_preidctors>255/2] <-1;
#For each digit in the mnist training data, compute the proportion of pixels that are in the grey area, defined as values between 50 and 205. (To visualize this, you can make a boxplot by digit class.)
gray_preidctors<- predictors
gray_preidctors[gray_preidctors>205 | gray_preidctors<50] <-0;
gray_preidctors[gray_preidctors<205 & gray_preidctors>50] <-1;
data.frame(labels=as.factor(labels), row_averages=rowMeans(gray_preidctors)) %>% ggplot(aes(labels,row_averages)) + geom_boxplot() + labs(title="Pixel darkness distribution by row")
mean(rowMeans(predictors<205 & predictors>50))
                
```
#Distance
Get some random samples of 2s and sevens
```{r}
set.seed(0)
sample_index <- which(mnist$train$labels %in% c(2,7)) %>% sample(500)
sample_predictors <- mnist$train$images[sample_index,]
sample_labels <- mnist$train$labels[sample_index]
sample_labels[1:3]
predictors_7_1 <- sample_predictors[1, ]
predictors_7_2 <- sample_predictors[2, ]
predictors_2_1 <- sample_predictors[3, ]
#checking distances between different predictors
sqrt(sum((predictors_7_1 - predictors_7_2)^2))
sqrt(sum((predictors_7_1 - predictors_2_1)^2))
sqrt(sum((predictors_7_2 - predictors_2_1)^2))
#get the distance automatically for all rows
image_distance <- dist(sample_predictors) %>% as.matrix()
#image(image_distance[order(sample_labels), order(sample_labels)])
```
##Excercise
```{r}
library(dslabs)
data("tissue_gene_expression")
#This dataset includes a matrix x:

dim(tissue_gene_expression$x)
#his matrix has the gene expression levels of 500 genes from 189 biological samples representing seven different tissues. The tissue type is stored in y:

table(tissue_gene_expression$y)
distances<- dist(tissue_gene_expression$x) 
#Compare the distances between observations 1 and 2 (both cerebellum), observations 39 and 40 (both colon), and observations 73 and 74 (both endometrium).

#Distance-wise, are samples from tissues of the same type closer to each other?
ind <- c(1, 2, 39, 40, 73, 74)
as.matrix(distances)[ind,ind]

```

#K nearest neigbours
```{r}
#Let's compare with the logistics regression
fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family="binomial")
p_hat_logistic<-predict(fit_glm, mnist_27$test)
y_hat_logistic<-factor(ifelse(p_hat_logistic > 0.5, 7, 2))
#Try and beat that
confusionMatrix(data = y_hat_logistic, reference=mnist_27$test$y)$overall[1]
#Trying...
knn_fit <- knn3( y ~ x_1 + x_2, data = mnist_27$train, k = 5) #k equals the neighbours 
y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(data = y_hat_knn, reference=mnist_27$test$y)

```

#K-fold Cross-validation
Remembering MSE: $$MSE = E\{\frac{1}{N} \sum_{i=1}^N(\widehat{Y}_i-Y_i)^2 \}$$

When we have only one data set, the MSE is estimated with the observed MSE as follows: $$\widehat{MSE} =E\{\frac{1}{N} \sum_{i=1}^N(\widehat{y}_i-y_i)^2 \} $$
Those quantities are reffered as true error and apparent error. (;

As the apparent error is a random variable, the true error might be considered as: $$TE = \sum_{b=1}^B\sum_{i=1}^N (\widehat{y}_i^b-y_i^b)^2 $$
Cross validation uses TE with the data we have, generating a series of random samples.

The k-fold cross validaion uses a training data set in order to estimate a best single parameter value (for example k in knn), simulating a random variable from an excerpt from the training set.
The set of parameters is referred as $\lambda$ .The obtained predicition from a single observation is denoted as $\widehat{y}_i(\lambda)$
$$MSE(\lambda) = \sum_{b=1}^B\sum_{i=1}^N (\widehat{y}_i^b(\lambda)-y_i^b)^2 $$

##Excercise 
```{r}
#Generate a set of random predictors using the following code:
set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep="_")
y <- rbinom(n, 1, 0.5) %>% factor()
x_subset <- x[, sample(p, 100)]

#Run cross validation using logistic regression to fit the  model
fit <- train(x_subset, y, method="glm")
fit$results

```
Now we will search for predictors that are the most predictive.
```{r}

tt <- colttests(x, y)
pvals <- tt$p.value
ind <- which(pvals<= 0.01)
x_subset <- x[, ind]

#Run cross validation using logistic regression to fit the  model
fit <- train(x_subset, y, method="glm")
fit$results

#Rerun cross validation using knn
fit <- train(x_subset, y, method="knn", tuneGrid=data.frame(k = seq(101,301,25)))
ggplot(fit)
```

```{r}
#Use the train function to predict tissue from gene expression in the tissue_gen_expression ds
library(dslabs)
data("tissue_gene_expression")
#This dataset includes a matrix x:
fit <- train(tissue_gene_expression$x, tissue_gene_expression$y, method="knn")
fit$results
```

#Bootstrap excercises
The createResample function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the mnist_27 dataset like this:
```{r}
set.seed(1995)
indexes <- createResample(mnist_27$tain$y, 10)
#How many times do 3, 4, and 7 appear in the first resampled index?
sum(indexes[[1]] == 3)
sum(indexes[[1]] == 4)
sum(indexes[[1]] == 7)

#How manu times do 3 appear in all resamples
x=sapply(indexes, function(ind){
	sum(ind == 3)
})
sum(x)
```
Generate a random dataset using the following code:

```{r}

#Estimate the 75th quantile, which we know is qnorm(0.75), with the sample quantile: quantile(y, 0.75).
set.seed(1)
B <- 10^5
y <- rnorm(100, 0, 1) 
  quantile_sd <- replicate(B, {
    y <- rnorm(100, 0, 1)  
    quantile(y, 0.75)
  })
  mean(quantile_sd)
  sd(quantile_sd)

#In practice, we can't run a Monte Carlo simulation. Use 10 bootstrap samples to estimate the standard error using just the initial sample y. Set the seed to 1.

indexes <- createResample(y, 10)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)

set.seed(1)
indexes <- createResample(y, 10000)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)

```
#Naive bayes
```{r}

library(caret)
#detach(package:plyr)
data("heights")
y <- heights$height
set.seed(2)
test_index <- createDataPartition(y, p= 0.5, list = FALSE)
test_set <- heights %>% slice(test_index)
train_set <- heights %>% slice(-test_index)
conditional_parameters <- train_set %>% group_by(sex) %>% summarize(avg=mean(height), sd=sd(height))
print("Heights training set conditional parameters are")
conditional_parameters
#getting the prevalence Y=1 (peopleS are female)
pi <- train_set %>% summarise(pi=mean(sex=="Female")) %>% .$pi
print("Prevalence is:")
pi
#f0 is fX|Y=0, f1 is fX|Y=1
x <- test_set$height
f0 <- dnorm(x, conditional_parameters$avg[2], conditional_parameters$sd[2])
f1 <- dnorm(x, conditional_parameters$avg[1], conditional_parameters$sd[1])
p_hat_bayes <- f1 * pi/(f1 * pi + f0 * (1-pi))

data.frame(x=x,p_hat=p_hat_bayes) %>% ggplot(aes(x,p_hat_bayes)) + geom_point() + geom_smooth()

#This sample is valid for our test and trainin sets, because Pi is ~0.23 in the overall data. 
#But applying this value to real life simulation will give skewed results due to low specificity.
#Naive bayes gives a direct way of fixing this. Replacing Pi's value! :D
p_hat_bayes_unbiased <- f1 * 0.5 /(f1 * 0.5 + f0 * 0.5)
y_hat_bayes_unbiased <- ifelse(p_hat_bayes_unbiased > 0.5, "Female", "Male")
#Let's check the specificity
print("Specificit")
sensitivity(data = factor(y_hat_bayes_unbiased), reference=factor(test_set$sex))

```
#QDA & LDA
```{r}
data("mnist_27")
#Let's get the conditional parameters
mnist_27_parameters <- mnist_27$train %>% group_by(y) %>% summarize(avg_1=mean(x_1), avg_2=mean(x_2), sd_1=sd(x_1), sd_2=mean(x_2), r=cor(x_1, x_2))
mnist_27_parameters
train_qda <- train(y ~., method="qda", data = mnist_27$train)
train_qda

mnist_27$train %>% mutate(y = factor(y)) %>% 
  ggplot(aes(x_1, x_2, fill = y, color=y))  + geom_point(show.legend = FALSE) + stat_ellipse(type="norm", lwd = 1.5)
y_hat <- predict(train_qda, mnist_27$test)
confusionMatrix(data = y_hat, reference = mnist_27$test$y)$overall["Accuracy"]
```

##Comprehension check

```{r}
set.seed(1993)
data("tissue_gene_expression")
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]
#Use the train function to estimate the accuracy of LDA.
train_set <- as.data.frame(x) %>% mutate(y=paste(y))
rownames(train_set) <- c()
train_lda <- train(y ~., method="lda", data = train_set)

#Repeat the exercise in Q1 with QDA.
#Create a dataset of samples from just cerebellum and hippocampus, two parts of the brain, and a predictor matrix with 10 randomly selected columns using the following code:
set.seed(1993)
data("tissue_gene_expression")
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]
train_set <- as.data.frame(x) %>% mutate(y=paste(y))
rownames(train_set) <- c()
train_qda <- train(y ~., method="qda", data = train_set, preProcessing = "scale")
t(train_qda$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(cerebellum, hippocampus, label = predictor_name)) +
	geom_point() +
	geom_text() +
	geom_abline()
```

```{r}

#Now we are going to increase the complexity of the challenge slightly: we will consider all the tissue types. Use the following code to create your dataset:

set.seed(1993)
data("tissue_gene_expression")
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x
x <- x[, sample(ncol(x), 10)]
train_set <- as.data.frame(x) %>% mutate(y=paste(y))
train_lda <- train(y ~., method="lda", data = train_set)

```
#Classification with more than two classes
##RTrees
```{r}
data("olive")
head(olive)
#Remove area, as we don't use it.
olive <- select(olive,-area)
#Let's use knn for determining Region over fatty acid content:
library(caret)
fit <- train(region ~ ., method="knn", tuneGrid=data.frame(k=seq(1,15,2)), data = olive)
ggplot(fit)
olive %>% gather(fatty_acid, percentage, -region) %>%
  ggplot(aes(region, percentage, fill = region)) +
  geom_boxplot() + 
  facet_wrap(~fatty_acid, scales = "free")

#Let's check out  linoleic/eicosenoic rate per region
p <- olive %>% 
  ggplot(aes(eicosenoic, linoleic, color = region)) + 
  geom_point()
#Let's examine 2008 polls:
data("polls_2008")
qplot(day, margin, data = polls_2008)
#Rpart lets R users (; partition according to RSS(Residual sum of squares)
fit_rpart <- rpart(margin ~ ., data=polls_2008)
plot(fit_rpart, margin=1)
text(fit_rpart, cex=0.5)
#Pruning let's acummulate branches of overtrained tree.
#Overtraining:
fit_unrpruned <- rpart(margin ~., data=polls_2008, cp=0, minsplit=2)
polls_2008 %>% mutate(y_hat = predict(fit_unrpruned)) %>% ggplot() + geom_point(aes(day, margin)) +   geom_step(aes(day, y_hat), col="red")
#Now let's prune and examine
fit_pruned <- prune(fit_unrpruned, cp=0.01)
polls_2008 %>% mutate(y_hat = predict(fit_pruned)) %>% ggplot() + geom_point(aes(day, margin)) +   geom_step(aes(day, y_hat), col="red")
#Examining Complexity parameter's best value can be done through cross-validation
train_rpart <-  train(margin ~ ., method="rpart", tuneGrid = data.frame(cp = seq(0,0.05, len=25)), data = polls_2008)
ggplot(train_rpart)
```
Expected conditional probability $$f(x) = \mbox{E}(Y | X = x)$$

###Excercise
```{r}
#Create a simple dataset where the outcome grows 0.75 units on average for every increase in a predictor, using this code:

library(rpart)
n <- 1000
sigma <- 0.25
set.seed(1)
x <- rnorm(n, 0, 1)
y <- 0.75 * x + rnorm(n, 0, sigma)
dat <- data.frame(x = x, y = y)
#use rpart to fit a regression tree and saves the result to fit
fit <- rpart(y ~ ., data = dat) 
plot(fit, margin=1)
text(fit, cex=0.5)
#make a scatter plot of y versus x along with the predicted values based on the fit.
dat %>% 
	mutate(y_hat = predict(fit)) %>% 
	ggplot() +
	geom_point(aes(x, y)) +
  geom_step(aes(x, y_hat), col=2)
```

#Decision trees
When outcome is categorical.
```{r}
#Let's play with the 2_7 dataset:
train_rpart <- train(y~., method="rpart", tuneGrid=data.frame(cp=seq(0.0,0.1, len=25)), data = mnist_27$train)
plot(train_rpart)
confusionMatrix(predict(train_rpart, mnist_27$test), mnist_27$test$y)$overall["Accuracy"]
#Let's plot 

plot_cond_prob <- function(p_hat=NULL){
     tmp <- mnist_27$true_p
     if(!is.null(p_hat)){
          tmp <- mutate(tmp, p=p_hat)
     }
     tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
          geom_raster(show.legend = FALSE) +
          scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
          stat_contour(breaks=c(0.5),color="black")
}
p1 <- plot_cond_prob(predict(train_rpart, newdata = mnist_27$true_p, type = "prob")[,2]) +
     ggtitle("Decision Tree")tuneGrid = data.frame(predFixed = seq(0, 3, 1), minNode = seq(25, 100, 25))
p1
```
#Random forests
```{r}
#"Trying with polls"
library(randomForest)
rf_fit <- randomForest(margin ~ ., data = polls_2008)
plot(rf_fit)
#2_7
rf_fit <- randomForest(y ~ ., data = mnist_27$train)
plot(rf_fit)

#Since the result is not yet smooth enough, I use Rborist as a random forest method to generate a cross-validated fit
rf_fit <- train (y ~ ., data = mnist_27$train, method="Rborist", tuneGrid = data.frame(predFixed = seq(0, 3, 1), minNode = seq(25, 100, 25)))
``` 

#Excercise
```{r}
library(randomForest)
fit <- randomForest(y ~ x, data = dat)
dat %>% 
	mutate(y_hat = predict(fit)) %>% 
	ggplot() +
	geom_point(aes(x, y)) +
	geom_step(aes(x, y_hat), col = 2)
plot(fit)
```
#Caret package playground
```{r}
library(caret)

dat %>% 
	mutate(y_hat = predict(fit)) %>% 
	ggplot() +
	geom_point(aes(x, y)) +
	geom_step(aes(x, y_hat), col = 2)
fit_rf <- train (y ~ ., data = dat, method="Rborist", tuneGrid = data.frame(predFixed = 1, minNode = seq(25, 100, 25)))
plot(fit_rf)
dat %>% 
	mutate(y_hat = predict(fit_rf)) %>% 
	ggplot() +
	geom_point(aes(x, y)) +
  geom_step(aes(x, y_hat), col = 2)
```
##Excercise
```{r}
#Use the rpart function to fit a classification tree to the tissue_gene_expression dataset. Use the train function to estimate the accuracy. Try out cp values of seq(0, 0.1, 0.01). Plot the accuracies to report the results of the best model. Set the seed to 1991. Which value of cp gives the highest accuracy?
library(caret)
set.seed(1991)
data("tissue_gene_expression")

fit_rpart <- with(tissue_gene_expression, train(x,y, method="rpart", tuneGrid = data.frame(cp = seq(0, 0.1, 0.01)), control=rpart.control(minsplit = 0)))
confusionMatrix(fit_rpart)
plot(fit_rpart$finalModel, margin=1)
text(fit_rpart$finalModel, cex=0.5)
#We can see that with just seven genes, we are able to predict the tissue type. Now let's see if we can predict the tissue type with even fewer genes using a Random Forest. Use the train function and the rf method to train a Random Forest. Try out values of mtry ranging from seq(50, 200, 25) (you can also explore other values on your own). What mtry value maximizes accuracy? To permit small nodesize to grow as we did with the classification trees, use the following argument: nodesize = 1. 
#Note: This exercise will take some time to run. If you want to test out your code first, try using smaller values with ntree. Set the seed to 1991 again.

#What value of mtry maximizes accuracy?
set.seed(1991)
fit_rpart<- with(tissue_gene_expression, train(x,y, method="rf", tuneGrid = data.frame(mtry = seq(50, 200, 25)), nodesize=1))

#The rpart model we ran above produced a tree that used just seven predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was fit_rpart, we can extract the names like this:
tree_terms <- as.character(unique(fit_rpart$finalModel$frame$var[!(fit_rpart$finalModel$frame$var == "<leaf>")]))
tree_terms
```

#MNIST case study
```{r}
library(dslabs)
mnist<-read_mnist()
#Let's sample a smaller set of data (computation wise)
set.seed(123)
index <- sample(nrow(mnist$train$images), 1000)
x<-mnist$train$images[index,]
y<-factor(mnist$train$labels[index])

index <- sample(nrow(mnist$train$images), 1000)
x_test<-mnist$train$images[index,]
y_test<-factor(mnist$train$labels[index])

library(matrixStats)
standard_deviations<-colSds(x)
qplot(standard_deviations, bins=256, color=I("black"))
nzv <- nearZeroVar(x)
image(matrix(1:784 %in% nzv, 28, 28))
#caret requires column names
colnames(x) <- 1:ncol(mnist$train$images) 
colnames(x_test) <- 1:ncol(mnist$train$images) 
col_index <- setdiff(1:ncol(x), nzv)
#Knn optimize for Nr neighoburs
control <- trainControl(method="cv", number= 9, p = .9)
fit_knn <- train(x[,col_index], y, method="knn", tuneGrid=data.frame(k=c(3,5,7)), trControl=control)
#Now we've found optimisation parameters:
fit_knn <- knn3(x[,col_index], y, k=3)
y_hat_knn <- predict(fit_knn, x_test[, col_index], type="class")
cm <- confusionMatrix(y_hat_knn, factor(y_test))
cm$overall["Accuracy"]
cm$byClass
```

```{r}
library(dslabs)
#mnist<-read_mnist()
#Let's sample a smaller set of data (computation wise)
set.seed(123)
index <- sample(nrow(mnist$train$images), 1000)
x<-mnist$train$images[index,]
y<-factor(mnist$train$labels[index])

index <- sample(nrow(mnist$train$images), 1000)
x_test<-mnist$train$images[index,]
y_test<-factor(mnist$train$labels[index])
#Time for random forest MNIST training
#Reducing compute time via max number of trees, minimum nodes = 1, predFixed is to be tuned
#caret requires column names
colnames(x) <- 1:ncol(mnist$train$images) 
colnames(x_test) <- 1:ncol(mnist$train$images) 
library(matrixStats)
nzv <- nearZeroVar(x)
col_index <- setdiff(1:ncol(x), nzv)
library(Rborist)
control <- trainControl(method="cv", number = 5, p = 0.8)
grid <- expand.grid(minNode = c(1) , predFixed = c(10, 15, 35))

train_rf <-  train(x[, col_index],
                   y,
                   method = "Rborist",
                   trControl = control)

ggplot(train_rf)
train_rf$bestTune

fit_rf <- Rborist(x[, col_index], y, 
                  nTree = 50,
                  minNode = train_rf$bestTune$minNode,
                  predFixed = train_rf$bestTune$predFixed)
y_hat_rf <- factor(levels(y)[predict(fit_rf, x_test[ ,col_index])$yPred])
cm <- confusionMatrix(y_hat_rf, y_test)
cm$overall["Accuracy"]
```
##Variable importance 
```{r}
set.seed(1)
library(randomForest)
index <- sample(nrow(mnist$train$images), 1000)
x<-mnist$train$images[index,]
y<-factor(mnist$train$labels[index])
rf<-randomForest(x,y,ntree=50)
imp <- importance(rf)
image(matrix(imp, 28, 28))
```

##Ensembles
```{r}
#Let's combine RF & KNN
pred_rf <- predict(fit_rf, x_test[,col_index])$census
```
##Excercises 
```{r}
#Use the training set to build a model with several of the models available from the caret package. We will test out all of the following models in this exercise:

models <- c("glm", "lda",  "naive_bayes",  "svmLinear", 
                "gamboost",  "gamLoess", "qda", 
                "knn", "kknn", "loclda", "gam",
                "rf", "ranger",  "wsrf", "Rborist", 
                "avNNet", "mlp", "monmlp",
                "adaboost", "gbm",
                "svmRadial", "svmRadialCost", "svmRadialSigma")
#We have not explained many of these, but apply them anyway using train with all the default parameters. You will likely need to install some packages. Keep in mind that you will probably get some warnings. Also, it will probably take a while to train all of the models - be patient!

#Run the following code to train the various models:

library(caret)
library(dslabs)
set.seed(1)
data("mnist_27")

fits <- lapply(models, function(model){ 
	print(model)
	train(y ~ ., method = model, data = mnist_27$train)
}) 
    
names(fits) <- models
pred <- sapply(fits, function(object) 
	predict(object, newdata = mnist_27$test))
dim(pred)
acc <- colMeans(pred == mnist_27$test$y)
acc
mean(acc)
```
```{r}
library(caret)
df <- data.frame(matrix(unlist(pred), nrow=23, byrow=T))
colnames(df) <- seq(1:200)
rownames(df) <- models
col_index <- seq(1,ncol(df), 1)
predict_vote_l <- map(col_index, function(j){
  ifelse(test = sum(df[,j] == 7) > 12, yes = 7, no = 2)
})
predict_vote <- as.factor(unlist(predict_vote_l)) #  as factor

caret::confusionMatrix(predict_vote,  mnist_27$test$y)

###Ahahaha and after seeeing the answer
votes <- rowMeans(pred == "7")
y_hat <- ifelse(votes > 0.5, "7", "2")
mean(y_hat == mnist_27$test$y)

#How many models perform better than the ensemble?
ind <- acc > mean(y_hat == mnist_27$test$y)
sum(ind)
models[ind]
```

```{r}
#It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from cross validation with the training data. Obtain these estimates and save them in an object. Report the mean accuracy of the new estimates.



acc <- sapply(fits, function(fit) {
  mean(fit[['results']][['Accuracy']])
})
acc
mean(acc)
```

```{r}
#Now let's only consider the methods with an estimated accuracy of greater than or equal to 0.8 when constructing the ensemble.
#What is the accuracy of the ensemble now?
model s<- unlist(sapply(fits, function(fit) {
  if(mean(fit[['results']][['Accuracy']]) > 0.8) {
    fit$method
  }
}))
fits <- lapply(models, function(model){ 
	print(model)
	train(y ~ ., method = model, data = mnist_27$train)
}) 
    
names(fits) <- models
pred <- sapply(fits, function(object) 
	predict(object, newdata = mnist_27$test))
dim(pred)
acc <- colMeans(pred == mnist_27$test$y)
acc
```
###Tissue gene expressions
```{r}
#We want to explore the tissue_gene_expression predictors by plotting them.
library(ggplot2)
data("tissue_gene_expression")
dim(tissue_gene_expression$x)
#We want to get an idea of which observations are close to each other, but, as you can see from the dimensions, the predictors are 500-dimensional, making plotting difficult. Plot the first two principal components with color representing tissue type. Which tissue is in a cluster by itself?
pc <- prcomp(tissue_gene_expression$x)
data.frame(pc_1 = pc$x[,1], pc_2 = pc$x[,2], 
			tissue = tissue_gene_expression$y)  %>%
	ggplot(aes(pc_1, pc_2, color = tissue)) +
	geom_point()
#The predictors for each observation are measured using the same device and experimental procedure. This introduces biases that can affect all the predictors from one observation. For each observation, compute the average across all predictors, and then plot this against the first PC with color representing tissue. Report the correlation.
```

```{r}
#What is the correlation
x <- with(tissue_gene_expression, sweep(x, 1, rowMeans(x)))
pc <- prcomp(x)
all_tissues <- rowMeans(data.frame(tissue_gene_expression$x))
all <-data.frame(pc_1 = pc$x[,1], pc_2 = pc$x[,2], 
			tissue = tissue_gene_expression$y, avg=all_tissues)
cor(all$pc_1, all$avg)
all %>% ggplot(aes(avg, pc_1, color = tissue)) +
	geom_point()

#We see an association with the first PC and the observation averages. Redo the PCA but only after removing the center. Part of the code is provided for you.

data.frame(pc_1 = pc$x[,1], pc_2 = pc$x[,2], 
			tissue = tissue_gene_expression$y) %>%
	ggplot(aes(pc_1, pc_2, color = tissue)) +
	geom_point()

#For the first 10 PCs, make a boxplot showing the values for each tissue. For the 7th PC, which two tissues have the greatest median difference?
allpcs<-data.frame(pc$x[,1:10], tissue=tissue_gene_expression$y)
allpcs$tissue=as.factor(allpcs$tissue)
ggplot(allpcs, aes(tissue, PC7)) + geom_boxplot()
for(i in 1:10){
	boxplot(pc$x[,i] ~ tissue_gene_expression$y, main = paste("PC", i))
}
```
#Recommendation systems

```{r}
library(dslabs)
data("movielens")
#How many users provided ratings for how many users
movielens %>% summarize(users = n_distinct(userId), movies = n_distinct(movieId))
##Using a smaller subset of movies, performancewise
keep <- movielens %>%
     dplyr::count(movieId) %>%
     top_n(5) %>%
     pull(movieId)
tab <- movielens %>%
     filter(userId %in% c(13:20)) %>% 
     filter(movieId %in% keep) %>% 
     select(userId, title, rating) %>% 
     spread(title, rating)
tab %>% knitr::kable()
#The spread
users <- sample(unique(movielens$userId), 100)
movielens %>% filter(userId %in% users) %>% 
select(userId, movieId, rating) %>%
mutate(rating = 1) %>%
spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
as.matrix() %>% t(.) %>%
image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```
Let's examine the data set.
```{r}
#Movie distribution
n<-0
movielens %>% 
     dplyr::count(movieId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 30, color = "black") + 
     scale_x_log10() + 
     ggtitle("Movies")
#Examine user activity
movielens %>% 
     dplyr::count(userId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 30, color = "black") + 
     scale_x_log10() + 
     ggtitle("Users")
```
Build a test and training set.
```{r}
library(caret)
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
#It is not enough to make a set negative, because users and movies might be missing from the training set.
test_set <- train_set %>% semi_join(train_set, by="movieId") %>% semi_join(train_set, by="userId")
```